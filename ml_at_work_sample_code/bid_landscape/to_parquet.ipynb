{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 5000000\n",
      "progress 10000000\n",
      "progress 15000000\n",
      "progress 20000000\n",
      "progress 25000000\n",
      "progress 30000000\n",
      "progress 35000000\n",
      "progress 40000000\n",
      "progress 45000000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "size = 50000000\n",
    "\n",
    "click = np.zeros(size, np.int8)\n",
    "market_price = np.zeros(size, np.int16)\n",
    "features = np.zeros([24, size], np.int64)\n",
    "\n",
    "\n",
    "with open('./dataset/train_set') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        raw = line.rstrip().split('\\t')\n",
    "        #print(raw)\n",
    "        click[i] = int(raw[0])\n",
    "        market_price[i] = int(raw[1])\n",
    "        for idx in range(24):\n",
    "            features[idx, i] = int(raw[idx+2].split(':')[0])\n",
    "\n",
    "            \n",
    "        i += 1\n",
    "        if i == size:\n",
    "            break\n",
    "        if i % 5000000 == 0:\n",
    "            print('progress', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(\n",
    "    features.T,\n",
    "    columns=['feature_'+str(n) for n in range(24)]\n",
    ")\n",
    "raw_df['click'] = click\n",
    "raw_df['market_price'] = market_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000000 entries, 0 to 49999999\n",
      "Data columns (total 26 columns):\n",
      " #   Column        Dtype\n",
      "---  ------        -----\n",
      " 0   feature_0     int64\n",
      " 1   feature_1     int64\n",
      " 2   feature_2     int64\n",
      " 3   feature_3     int64\n",
      " 4   feature_4     int64\n",
      " 5   feature_5     int64\n",
      " 6   feature_6     int64\n",
      " 7   feature_7     int64\n",
      " 8   feature_8     int64\n",
      " 9   feature_9     int64\n",
      " 10  feature_10    int64\n",
      " 11  feature_11    int64\n",
      " 12  feature_12    int64\n",
      " 13  feature_13    int64\n",
      " 14  feature_14    int64\n",
      " 15  feature_15    int64\n",
      " 16  feature_16    int64\n",
      " 17  feature_17    int64\n",
      " 18  feature_18    int64\n",
      " 19  feature_19    int64\n",
      " 20  feature_20    int64\n",
      " 21  feature_21    int64\n",
      " 22  feature_22    int64\n",
      " 23  feature_23    int64\n",
      " 24  click         int8 \n",
      " 25  market_price  int16\n",
      "dtypes: int16(1), int64(24), int8(1)\n",
      "memory usage: 9.1 GB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_parquet('./raw_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parquet_with_split(df, file_path_template):\n",
    "    partial_size = 5000000\n",
    "    size = len(df)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        start = partial_size * idx\n",
    "        end = start + partial_size\n",
    "        _df = df[start:end]\n",
    "        file_name = file_path_template.format(idx)\n",
    "        _df.to_parquet(file_name)\n",
    "        print(file_name, start, end)\n",
    "        idx += 1\n",
    "        if end > size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_template = \"./to_upload/raw_{}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./to_upload/raw_0.parquet 0 5000000\n",
      "./to_upload/raw_1.parquet 5000000 10000000\n",
      "./to_upload/raw_2.parquet 10000000 15000000\n",
      "./to_upload/raw_3.parquet 15000000 20000000\n",
      "./to_upload/raw_4.parquet 20000000 25000000\n",
      "./to_upload/raw_5.parquet 25000000 30000000\n",
      "./to_upload/raw_6.parquet 30000000 35000000\n",
      "./to_upload/raw_7.parquet 35000000 40000000\n",
      "./to_upload/raw_8.parquet 40000000 45000000\n",
      "./to_upload/raw_9.parquet 45000000 50000000\n",
      "./to_upload/raw_10.parquet 50000000 55000000\n"
     ]
    }
   ],
   "source": [
    "to_parquet_with_split(raw_df, path_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.0G\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Jan 14 18:38 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4.0K Jan 14 18:39 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_0.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_1.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter  14K Jan 14 18:38 raw_10.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_2.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 202M Jan 14 18:38 raw_3.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_4.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 202M Jan 14 18:38 raw_5.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_6.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 202M Jan 14 18:38 raw_7.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_8.parquet\n",
      "-rw-r--r-- 1 jupyter jupyter 203M Jan 14 18:38 raw_9.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ./to_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./to_upload/raw_0.parquet [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./to_upload/raw_1.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_10.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_2.parquet [Content-Type=application/octet-stream]...\n",
      "| [4 files][607.1 MiB/607.1 MiB]   42.3 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://./to_upload/raw_3.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_4.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_5.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_6.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_7.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_8.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file://./to_upload/raw_9.parquet [Content-Type=application/octet-stream]...\n",
      "| [11 files][  2.0 GiB/  2.0 GiB]   41.6 MiB/s                                  \n",
      "Operation completed over 11 objects/2.0 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./to_upload/*.parquet gs://xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r71272dc5e6da8a0d_00000177027b1dfd_1 ... (110s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq load --source_format=PARQUET investigation_tnishibayashi.yoyi_train gs://xxxxxx/upload/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
