{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "発表者@hagino3000です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 勾配降下法\n",
    "\n",
    "順伝播型ネットワークの学習、は与えられた訓練データ(入力と出力のペア)\n",
    "\n",
    "$D = \\{(x_1, d_1),...,(x_N,d_N)\\}$\n",
    "\n",
    "を元に計算される誤差関数 $E(w)$ をネットワークのパラメータ$w$について最小化すること。\n",
    "\n",
    "誤差関数は回帰問題では\n",
    "\n",
    "$\\displaystyle\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}||d_n - y(x_n;w)||^2$\n",
    "\n",
    "多クラス問題では (式 2.11の交差エントロピー)\n",
    "\n",
    "$\\displaystyle\n",
    "E(w) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}d_{nk}log_{yk}(x_n;w)$\n",
    "\n",
    "学習のゴールは $E(w)$ を最小にする $w = argmin_wE(w)$ を求める事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### どのようにして $w = argmin_wE(w)$ を求めるか\n",
    "\n",
    "#### 一般的に大域的な最小解を直接求めるのが不可能\n",
    "\n",
    "- 一般に$E(w)$は凸関数では無い\n",
    "- 極小点が多数存在する\n",
    "- 大域的な極小点を求める必要がある\n",
    "\n",
    "何らかの初期値を出発点に $w$ を繰り返し更新する反復計算によって求める。\n",
    "\n",
    "→ 勾配降下法 (gradient descent method)\n",
    "\n",
    "勾配\n",
    "\n",
    "$\\displaystyle\n",
    "\\nabla{E} \\equiv \\frac{dE}{dw} = [\\frac{dE}{dw_1} \\dots \\frac{dE}{dw_M}]$\n",
    "\n",
    "$t + 1$回$w$を動かした後の重みは\n",
    "\n",
    "$w^{(t + 1)} = w^{(t)} - \\epsilon\\nabla{E}$\n",
    "\n",
    "となる。 $\\epsilon$ は学習係数。\n",
    "\n",
    "----\n",
    "\n",
    "_補足_  \n",
    "**直接求められる**ケースというのは、誤差関数を微分してゼロになる点を求めればOKという場合。  \n",
    "本で見るけど実際にやった事は無いです\n",
    "\n",
    "\n",
    "### その他のより収束の早い最小化手法\n",
    "\n",
    "- ニュートン法\n",
    "- ガウスニュートン法\n",
    "- etc\n",
    "\n",
    "しかしニュートン法とその派生手法は2次の微分を求める必要があり、問題の規模が大きいと難しい。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 確率的勾配降下法\n",
    "\n",
    "全てのサンプルに対して計算される誤差を最小化 → バッチ処理\n",
    "\n",
    "$\\displaystyle\n",
    "E(w) = \\sum_{n=1}^{N}E_n(w)$\n",
    "\n",
    "$w^{(t+1)} = w^{(t)} + \\epsilon\\nabla{E_n}$\n",
    "\n",
    "サンプル一つだけを使って誤差を計算して勾配方向にパラメータを更新 → 確率的勾配降下法(stochastic gradient descent)\n",
    "\n",
    "$\\displaystyle\n",
    "E(w) = E_n(w)$\n",
    "\n",
    "$w^{(t+1)} = w^{(t)} + \\epsilon\\nabla{E_n}$\n",
    "\n",
    "確率的勾配降下法のメリット\n",
    "\n",
    "- 局所解に嵌る事がない\n",
    "- 学習の途中経過をより細かく監視できる\n",
    "- オンライン学習に利用できる\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ミニバッチ\n",
    "\n",
    "少数のサンプル集合$D_t$に対して誤差を計算して勾配方向にパラメータを更新する\n",
    "\n",
    "$\\displaystyle\n",
    "E_t(w) = \\frac{1}{N_t}\\sum_{n \\in D_{t}}E_n(w)$\n",
    "\n",
    "\n",
    "### ミニバッチのサイズの決め方\n",
    "\n",
    "- 系統的なやり方はない\n",
    "- ミニバッチそれぞれに各クラスから1つ以上のサンプルを入れるのが理想\n",
    "- 10~100サンプル前後とする事が多い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 汎化性能と過適合\n",
    "\n",
    "過学習の話"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 過適合の緩和\n",
    "\n",
    "\n",
    "### 3.5.2 正則化\n",
    "\n",
    "誤差関数に重みの二乗和を追加 → 重み減衰\n",
    "\n",
    "$\\displaystyle\n",
    "E_{t}(w) \\equiv \\frac{1}{N_t}\\sum_{n \\in D_{t}}E_n(w) + \\frac{\\lambda}{2}||w||^2$\n",
    "\n",
    "$w$の更新式は次となり、更新量が減る\n",
    "\n",
    "$\\displaystyle\n",
    "w^{(t+1)} = w^{(t)} - \\epsilon(\\frac{1}{N_t}\\sum\\nabla E_n + \\lambda w^{(t)})$\n",
    "\n",
    "重みの大きさの上限を制約する方法 → 重み上限\n",
    "\n",
    "$\\displaystyle\n",
    "\\sum_{i}w_{ji}^2 < c$ を満たすように制約をする。\n",
    "\n",
    "### 3.5.3 ドロップアウト\n",
    "\n",
    "ここはニューラルネット特有の話\n",
    "\n",
    "方法\n",
    "\n",
    "- 学習時に中間層の各層と入力層のユニットを決まった確率pでランダムに選出し、それ以外を無効化\n",
    "- 終了時の推論ではすべてのユニットを使って順伝播計算を行なう\n",
    "\n",
    "狙い\n",
    "\n",
    "- 学習時のネットワークの自由度を減らして、過適合を減らす\n",
    "- 複数のネットワークの平均を取る事になる → 推論の精度があがる → 3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 学習のトリック\n",
    "\n",
    "### 3.6.1 データの正規化\n",
    "\n",
    "サンプル平均がゼロ、分散が1になるようにする\n",
    "\n",
    "### 3.6.2 データ拡張\n",
    "\n",
    "手持ちのサンプルデータに何らかの加工を施し、量を「水増し」する。\n",
    "\n",
    "### 3.6.3 複数ネットの平均\n",
    "\n",
    "同じ入力を複数のニューラルネットに与え、得られる複数の平均を答えとする → モデル平均\n",
    "\n",
    "- 構造が異なるネットワークを同じ訓練データで訓練した物\n",
    "- 同じ構造のネットワークであっても、学習開始時の初期値を変えて同じ訓練データで訓練した物\n",
    "\n",
    "### 3.6.4 学習係数の決め方\n",
    "\n",
    "定盤の考え方が二つある\n",
    "\n",
    "1. 学習の初期ほど大きな値を選び、学習の進捗とともに学習係数を小さくする\n",
    "2. 層ごとに異なる値を使う\n",
    "\n",
    "学習係数を自動的に定める方法 **AdaGrad**\n",
    "\n",
    "勾配$\\nabla E_t$の成分を $\\displaystyle -\\frac{\\epsilon}{\\sqrt{\\sum_{t'=1}^{t}g_{t',i}^{2}}}g_{t,i}$ とし、頻出する勾配成分は減らし、まれに現われる成分を重視\n",
    "\n",
    "### 3.6.5 モメンタム (momentum)\n",
    "\n",
    "勾配降下法の収束性能を向上させる方法\n",
    "\n",
    "ミニバッチ $t - 1$ に対する重みの修正量を\n",
    "\n",
    "$\\nabla w^{(t-1)} \\equiv w^{(t-1)} - w^{(t-2)}$ \n",
    "\n",
    "とすると、ミニバッチ $t$ に対する更新を\n",
    "\n",
    "$w^{(t+1)} = w^{(t)} - \\epsilon\\nabla E_t + \\mu\\Delta w^{(t-1)}$\n",
    "\n",
    "                      ~~~~~~~ (追加した項)\n",
    "                      \n",
    "谷底の高低差が無い場合、勾配降下法はパフォーマンスが悪くなるが、効率的に探索できるようになる。  \n",
    "\n",
    "__補足__  \n",
    "ニュートン法も同じ問題を解決している。\n",
    "\n",
    "### 3.6.6 重みの初期化\n",
    "\n",
    "一般的な方法はガウス分布から生成したランダム値を初期値とする方法。\n",
    "\n",
    "\n",
    "### 3.6.7 サンプルの順序\n",
    "\n",
    "訓練サンプルをどのような順序で取り出すか。見なれないサンプルを先に学習させると効果的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
